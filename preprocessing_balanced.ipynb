{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert JSON to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File E:\\AIEBA Datasets\\Proj1 Datasets\\GoogleRestaurantReview.json does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [1], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Since google restaurant dataset is in json format, use pandas to convert it into a CSV file\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39m# DO NOT RERUN\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m \n\u001b[1;32m----> 4\u001b[0m df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_json(\u001b[39mr\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mE:\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mAIEBA Datasets\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mProj1 Datasets\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mGoogleRestaurantReview.json\u001b[39;49m\u001b[39m'\u001b[39;49m, lines\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m      5\u001b[0m df\u001b[39m.\u001b[39mto_csv(\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m\u001b[39mE:\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mAIEBA Datasets\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mProj1 Datasets\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mGoogleRestaurantReview.csv\u001b[39m\u001b[39m'\u001b[39m, index \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\czy20\\anaconda3\\envs\\myenv\\lib\\site-packages\\pandas\\util\\_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[39m=\u001b[39m new_arg_value\n\u001b[1;32m--> 211\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\czy20\\anaconda3\\envs\\myenv\\lib\\site-packages\\pandas\\util\\_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[0;32m    326\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    327\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    328\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[0;32m    329\u001b[0m         stacklevel\u001b[39m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    330\u001b[0m     )\n\u001b[1;32m--> 331\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\czy20\\anaconda3\\envs\\myenv\\lib\\site-packages\\pandas\\io\\json\\_json.py:733\u001b[0m, in \u001b[0;36mread_json\u001b[1;34m(path_or_buf, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, numpy, precise_float, date_unit, encoding, encoding_errors, lines, chunksize, compression, nrows, storage_options)\u001b[0m\n\u001b[0;32m    730\u001b[0m \u001b[39mif\u001b[39;00m convert_axes \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m orient \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtable\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    731\u001b[0m     convert_axes \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m--> 733\u001b[0m json_reader \u001b[39m=\u001b[39m JsonReader(\n\u001b[0;32m    734\u001b[0m     path_or_buf,\n\u001b[0;32m    735\u001b[0m     orient\u001b[39m=\u001b[39;49morient,\n\u001b[0;32m    736\u001b[0m     typ\u001b[39m=\u001b[39;49mtyp,\n\u001b[0;32m    737\u001b[0m     dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[0;32m    738\u001b[0m     convert_axes\u001b[39m=\u001b[39;49mconvert_axes,\n\u001b[0;32m    739\u001b[0m     convert_dates\u001b[39m=\u001b[39;49mconvert_dates,\n\u001b[0;32m    740\u001b[0m     keep_default_dates\u001b[39m=\u001b[39;49mkeep_default_dates,\n\u001b[0;32m    741\u001b[0m     numpy\u001b[39m=\u001b[39;49mnumpy,\n\u001b[0;32m    742\u001b[0m     precise_float\u001b[39m=\u001b[39;49mprecise_float,\n\u001b[0;32m    743\u001b[0m     date_unit\u001b[39m=\u001b[39;49mdate_unit,\n\u001b[0;32m    744\u001b[0m     encoding\u001b[39m=\u001b[39;49mencoding,\n\u001b[0;32m    745\u001b[0m     lines\u001b[39m=\u001b[39;49mlines,\n\u001b[0;32m    746\u001b[0m     chunksize\u001b[39m=\u001b[39;49mchunksize,\n\u001b[0;32m    747\u001b[0m     compression\u001b[39m=\u001b[39;49mcompression,\n\u001b[0;32m    748\u001b[0m     nrows\u001b[39m=\u001b[39;49mnrows,\n\u001b[0;32m    749\u001b[0m     storage_options\u001b[39m=\u001b[39;49mstorage_options,\n\u001b[0;32m    750\u001b[0m     encoding_errors\u001b[39m=\u001b[39;49mencoding_errors,\n\u001b[0;32m    751\u001b[0m )\n\u001b[0;32m    753\u001b[0m \u001b[39mif\u001b[39;00m chunksize:\n\u001b[0;32m    754\u001b[0m     \u001b[39mreturn\u001b[39;00m json_reader\n",
      "File \u001b[1;32mc:\\Users\\czy20\\anaconda3\\envs\\myenv\\lib\\site-packages\\pandas\\io\\json\\_json.py:818\u001b[0m, in \u001b[0;36mJsonReader.__init__\u001b[1;34m(self, filepath_or_buffer, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, numpy, precise_float, date_unit, encoding, lines, chunksize, compression, nrows, storage_options, encoding_errors)\u001b[0m\n\u001b[0;32m    815\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlines:\n\u001b[0;32m    816\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mnrows can only be passed if lines=True\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 818\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_data_from_filepath(filepath_or_buffer)\n\u001b[0;32m    819\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_preprocess_data(data)\n",
      "File \u001b[1;32mc:\\Users\\czy20\\anaconda3\\envs\\myenv\\lib\\site-packages\\pandas\\io\\json\\_json.py:874\u001b[0m, in \u001b[0;36mJsonReader._get_data_from_filepath\u001b[1;34m(self, filepath_or_buffer)\u001b[0m\n\u001b[0;32m    866\u001b[0m     filepath_or_buffer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles\u001b[39m.\u001b[39mhandle\n\u001b[0;32m    867\u001b[0m \u001b[39melif\u001b[39;00m (\n\u001b[0;32m    868\u001b[0m     \u001b[39misinstance\u001b[39m(filepath_or_buffer, \u001b[39mstr\u001b[39m)\n\u001b[0;32m    869\u001b[0m     \u001b[39mand\u001b[39;00m filepath_or_buffer\u001b[39m.\u001b[39mlower()\u001b[39m.\u001b[39mendswith(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    872\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m file_exists(filepath_or_buffer)\n\u001b[0;32m    873\u001b[0m ):\n\u001b[1;32m--> 874\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mFileNotFoundError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFile \u001b[39m\u001b[39m{\u001b[39;00mfilepath_or_buffer\u001b[39m}\u001b[39;00m\u001b[39m does not exist\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    876\u001b[0m \u001b[39mreturn\u001b[39;00m filepath_or_buffer\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: File E:\\AIEBA Datasets\\Proj1 Datasets\\GoogleRestaurantReview.json does not exist"
     ]
    }
   ],
   "source": [
    "# Since google restaurant dataset is in json format, use pandas to convert it into a CSV file\n",
    "# DO NOT RERUN\n",
    "import pandas as pd \n",
    "df = pd.read_json(r'E:\\AIEBA Datasets\\Proj1 Datasets\\GoogleRestaurantReview.json', lines=True)\n",
    "df.to_csv(r'E:\\AIEBA Datasets\\Proj1 Datasets\\GoogleRestaurantReview.csv', index = None)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in all three datasets\n",
    "# .csv files use the utf-8 encodings\n",
    "import pandas as pd\n",
    "\n",
    "amzn = pd.read_csv(r'E:\\AIEBA Datasets\\Proj1 Datasets\\AmazonReviews.csv',encoding='utf-8',engine='python')\n",
    "googl = pd.read_csv(r'E:\\AIEBA Datasets\\Proj1 Datasets\\GoogleRestaurantReview.csv',encoding='utf-8',engine='python')\n",
    "dis = pd.read_csv(r'E:\\AIEBA Datasets\\Proj1 Datasets\\DisneylandReviews.csv',encoding='utf-8',engine='python')\n",
    "\n",
    "# amzn = pd.read_csv(r'C:\\AIEBA Datasets\\Proj1 Datasets\\AmazonReviews.csv',encoding='utf-8',engine='python')\n",
    "# googl = pd.read_csv(r'C:\\AIEBA Datasets\\Proj1 Datasets\\GoogleRestaurantReview.csv',encoding='utf-8',engine='python')\n",
    "# dis = pd.read_csv(r'C:\\AIEBA Datasets\\Proj1 Datasets\\DisneylandReviews.csv',encoding='utf-8',engine='python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amzn.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "googl.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dis.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviews_rating</th>\n",
       "      <th>reviews_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.0</td>\n",
       "      <td>This $product so far has not%&amp;^# disappointed....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.0</td>\n",
       "      <td>great for beginner or experienced person. Boug...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.0</td>\n",
       "      <td>Inexpensive tablet for him to use and learn on...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.0</td>\n",
       "      <td>I've had my Fire HD 8 two weeks now and I love...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>I bought this for my grand daughter when she c...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   reviews_rating                                       reviews_text\n",
       "0             5.0  This $product so far has not%&^# disappointed....\n",
       "1             5.0  great for beginner or experienced person. Boug...\n",
       "2             5.0  Inexpensive tablet for him to use and learn on...\n",
       "3             4.0  I've had my Fire HD 8 two weeks now and I love...\n",
       "4             5.0  I bought this for my grand daughter when she c..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop columns that are not needed and standardize column names\n",
    "amzn = amzn.filter(items=['reviews.rating', 'reviews.text'])\n",
    "amzn = amzn.rename(columns={\"reviews.rating\":\"reviews_rating\", \"reviews.text\":\"reviews_text\"})\n",
    "\n",
    "googl = googl.filter(items=['rating', 'review_text'])\n",
    "googl = googl.rename(columns={\"rating\":\"reviews_rating\", \"review_text\":\"reviews_text\"})\n",
    "\n",
    "dis = dis.filter(items=['Rating','Review_Text'])\n",
    "dis = dis.rename(columns={\"Rating\":\"reviews_rating\", \"Review_Text\":\"reviews_text\"})\n",
    "\n",
    "amzn.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviews_rating</th>\n",
       "      <th>reviews_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.0</td>\n",
       "      <td>This $product so far has not%&amp;^# disappointed....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.0</td>\n",
       "      <td>great for beginner or experienced person. Boug...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.0</td>\n",
       "      <td>Inexpensive tablet for him to use and learn on...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.0</td>\n",
       "      <td>I've had my Fire HD 8 two weeks now and I love...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>I bought this for my grand daughter when she c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1125886</th>\n",
       "      <td>5.0</td>\n",
       "      <td>i went to disneyland paris in july 03 and thou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1125887</th>\n",
       "      <td>5.0</td>\n",
       "      <td>2 adults and 1 child of 11 visited Disneyland ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1125888</th>\n",
       "      <td>5.0</td>\n",
       "      <td>My eleven year old daughter and myself went to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1125889</th>\n",
       "      <td>4.0</td>\n",
       "      <td>This hotel, part of the Disneyland Paris compl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1125890</th>\n",
       "      <td>4.0</td>\n",
       "      <td>I went to the Disneyparis resort, in 1996, wit...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1125891 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         reviews_rating                                       reviews_text\n",
       "0                   5.0  This $product so far has not%&^# disappointed....\n",
       "1                   5.0  great for beginner or experienced person. Boug...\n",
       "2                   5.0  Inexpensive tablet for him to use and learn on...\n",
       "3                   4.0  I've had my Fire HD 8 two weeks now and I love...\n",
       "4                   5.0  I bought this for my grand daughter when she c...\n",
       "...                 ...                                                ...\n",
       "1125886             5.0  i went to disneyland paris in july 03 and thou...\n",
       "1125887             5.0  2 adults and 1 child of 11 visited Disneyland ...\n",
       "1125888             5.0  My eleven year old daughter and myself went to...\n",
       "1125889             4.0  This hotel, part of the Disneyland Paris compl...\n",
       "1125890             4.0  I went to the Disneyparis resort, in 1996, wit...\n",
       "\n",
       "[1125891 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Concatenate all 3 datasets by column\n",
    "combined = pd.concat([amzn, googl, dis], ignore_index=True)\n",
    "combined"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleansing and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reviews_rating    55\n",
      "reviews_text      16\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# remove rows with missing values\n",
    "print(combined.isnull().sum())\n",
    "combined = combined.dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#missing values removed \n",
    "print(combined.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined[\"reviews_rating\"].value_counts().plot.bar()\n",
    "combined[\"reviews_rating\"].value_counts(ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviews_rating</th>\n",
       "      <th>reviews_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18203</th>\n",
       "      <td>5.0</td>\n",
       "      <td>Super convenient. Battery life for weeks. Perf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14901</th>\n",
       "      <td>5.0</td>\n",
       "      <td>My children love these kindles! They are so mu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>344876</th>\n",
       "      <td>5.0</td>\n",
       "      <td>I have to say, I had a few choices to choose f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>540077</th>\n",
       "      <td>5.0</td>\n",
       "      <td>Phenomenal BBQ combined with fast service and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1024969</th>\n",
       "      <td>5.0</td>\n",
       "      <td>The beef noodle soup is the star. You can't ge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>552208</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Donâ€™t waste time or money!!!!!!! I live 3 mi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1069965</th>\n",
       "      <td>1.0</td>\n",
       "      <td>(Translated by Google) We went here to eat the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>484556</th>\n",
       "      <td>1.0</td>\n",
       "      <td>We were anticipating a delightful experience a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>401200</th>\n",
       "      <td>1.0</td>\n",
       "      <td>128th Denny's is officially ghetto\\nHave been ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1072684</th>\n",
       "      <td>1.0</td>\n",
       "      <td>When I asked if I could substitute on the meal...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150460 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         reviews_rating                                       reviews_text\n",
       "18203               5.0  Super convenient. Battery life for weeks. Perf...\n",
       "14901               5.0  My children love these kindles! They are so mu...\n",
       "344876              5.0  I have to say, I had a few choices to choose f...\n",
       "540077              5.0  Phenomenal BBQ combined with fast service and ...\n",
       "1024969             5.0  The beef noodle soup is the star. You can't ge...\n",
       "...                 ...                                                ...\n",
       "552208              1.0  Donâ€™t waste time or money!!!!!!! I live 3 mi...\n",
       "1069965             1.0  (Translated by Google) We went here to eat the...\n",
       "484556              1.0  We were anticipating a delightful experience a...\n",
       "401200              1.0  128th Denny's is officially ghetto\\nHave been ...\n",
       "1072684             1.0  When I asked if I could substitute on the meal...\n",
       "\n",
       "[150460 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAD+CAYAAAA6c3LAAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAS9klEQVR4nO3df6zddX3H8efL1jGmA/lxYaQtlkizWdgso6ldXBZcl1F1WTGB5PKH7R/daljNNDFLwP2h+6OL/KFkJMKsg1GIEzrU0Ci4EdAZMyxeHLMUJN4JwrUVroKAU3At7/1xPnc9vZzee+697T236/ORfHO+5/39fr79fD89p6/z/XF6UlVIkvS6QXdAkrQwGAiSJMBAkCQ1BoIkCTAQJEnN4kF3YLbOPPPMWr58+aC7IUnHlYceeujHVTXUa9lxGwjLly9nZGRk0N2QpONKkh8caZmnjCRJgIEgSWoMBEkSYCBIkhoDQZIEGAiSpGbaQEjyq0keTPKfSfYm+ZtWPz3JvUm+1x5P62pzTZLRJI8nubSrfnGSPW3Z9UnS6icluaPVdydZfgz2VZI0hX6OEF4B/rCq3gasAtYnWQtcDdxXVSuA+9pzkqwEhoELgPXADUkWtW3dCGwBVrRpfatvBp6vqvOB64Br575rkqSZmDYQquNn7enr21TABmBHq+8ALmvzG4Dbq+qVqnoCGAXWJDkHOKWqHqjOjzDcOqnNxLbuBNZNHD1IkuZHX99Ubp/wHwLOBz5VVbuTnF1V+wGqan+Ss9rqS4BvdjUfa7X/afOT6xNtnm7bOpDkBeAM4MeT+rGFzhEG5557br/7eETLr/7ynLcxV09+/D2D7gLgWHRzLA5xLA45Ecair4vKVXWwqlYBS+l82r9witV7fbKvKepTtZncj+1VtbqqVg8N9fyvOCRJszSju4yq6qfA1+ic+3+mnQaiPT7bVhsDlnU1Wwrsa/WlPeqHtUmyGDgVeG4mfZMkzU0/dxkNJXlTmz8Z+CPgu8AuYFNbbRNwV5vfBQy3O4fOo3Px+MF2eumlJGvb9YGNk9pMbOty4P7yx54laV71cw3hHGBHu47wOmBnVX0pyQPAziSbgaeAKwCqam+SncCjwAFga1UdbNu6CrgFOBm4p00ANwG3JRmlc2QwfDR2TpLUv2kDoaq+A1zUo/4TYN0R2mwDtvWojwCvuf5QVS/TAkWSNBh+U1mSBBgIkqTGQJAkAQaCJKkxECRJgIEgSWoMBEkSYCBIkhoDQZIEGAiSpMZAkCQBBoIkqTEQJEmAgSBJagwESRJgIEiSGgNBkgQYCJKkxkCQJAEGgiSpMRAkSYCBIElqDARJEmAgSJIaA0GSBBgIkqRm2kBIsizJV5M8lmRvkg+2+seS/DDJw216d1eba5KMJnk8yaVd9YuT7GnLrk+SVj8pyR2tvjvJ8mOwr5KkKfRzhHAA+HBVvRVYC2xNsrItu66qVrXpboC2bBi4AFgP3JBkUVv/RmALsKJN61t9M/B8VZ0PXAdcO/ddkyTNxLSBUFX7q+rbbf4l4DFgyRRNNgC3V9UrVfUEMAqsSXIOcEpVPVBVBdwKXNbVZkebvxNYN3H0IEmaHzO6htBO5VwE7G6lDyT5TpKbk5zWakuAp7uajbXakjY/uX5Ym6o6ALwAnNHjz9+SZCTJyPj4+Ey6LkmaRt+BkOSNwOeBD1XVi3RO/7wFWAXsBz4xsWqP5jVFfao2hxeqtlfV6qpaPTQ01G/XJUl96CsQkryeThh8tqq+AFBVz1TVwap6FfgMsKatPgYs62q+FNjX6kt71A9rk2QxcCrw3Gx2SJI0O/3cZRTgJuCxqvpkV/2crtXeCzzS5ncBw+3OofPoXDx+sKr2Ay8lWdu2uRG4q6vNpjZ/OXB/u84gSZoni/tY5x3A+4A9SR5utY8AVyZZRefUzpPA+wGqam+SncCjdO5Q2lpVB1u7q4BbgJOBe9oEncC5LckonSOD4bnslCRp5qYNhKr6Br3P8d89RZttwLYe9RHgwh71l4ErpuuLJOnY8ZvKkiTAQJAkNQaCJAkwECRJjYEgSQIMBElSYyBIkgADQZLUGAiSJMBAkCQ1BoIkCTAQJEmNgSBJAgwESVJjIEiSAANBktQYCJIkwECQJDUGgiQJMBAkSY2BIEkCDARJUmMgSJIAA0GS1BgIkiSgj0BIsizJV5M8lmRvkg+2+ulJ7k3yvfZ4Wleba5KMJnk8yaVd9YuT7GnLrk+SVj8pyR2tvjvJ8mOwr5KkKfRzhHAA+HBVvRVYC2xNshK4GrivqlYA97XntGXDwAXAeuCGJIvatm4EtgAr2rS+1TcDz1fV+cB1wLVHYd8kSTMwbSBU1f6q+nabfwl4DFgCbAB2tNV2AJe1+Q3A7VX1SlU9AYwCa5KcA5xSVQ9UVQG3Tmozsa07gXUTRw+SpPkxo2sI7VTORcBu4Oyq2g+d0ADOaqstAZ7uajbWakva/OT6YW2q6gDwAnBGjz9/S5KRJCPj4+Mz6bokaRp9B0KSNwKfBz5UVS9OtWqPWk1Rn6rN4YWq7VW1uqpWDw0NTddlSdIM9BUISV5PJww+W1VfaOVn2mkg2uOzrT4GLOtqvhTY1+pLe9QPa5NkMXAq8NxMd0aSNHv93GUU4Cbgsar6ZNeiXcCmNr8JuKurPtzuHDqPzsXjB9tppZeSrG3b3DipzcS2Lgfub9cZJEnzZHEf67wDeB+wJ8nDrfYR4OPAziSbgaeAKwCqam+SncCjdO5Q2lpVB1u7q4BbgJOBe9oEncC5LckonSOD4bntliRppqYNhKr6Br3P8QOsO0KbbcC2HvUR4MIe9ZdpgSJJGgy/qSxJAgwESVJjIEiSAANBktQYCJIkwECQJDUGgiQJMBAkSY2BIEkCDARJUmMgSJIAA0GS1BgIkiTAQJAkNQaCJAkwECRJjYEgSQIMBElSYyBIkgADQZLUGAiSJMBAkCQ1BoIkCTAQJEmNgSBJAvoIhCQ3J3k2ySNdtY8l+WGSh9v07q5l1yQZTfJ4kku76hcn2dOWXZ8krX5SkjtafXeS5Ud5HyVJfejnCOEWYH2P+nVVtapNdwMkWQkMAxe0NjckWdTWvxHYAqxo08Q2NwPPV9X5wHXAtbPcF0nSHEwbCFX1deC5Pre3Abi9ql6pqieAUWBNknOAU6rqgaoq4Fbgsq42O9r8ncC6iaMHSdL8mcs1hA8k+U47pXRaqy0Bnu5aZ6zVlrT5yfXD2lTVAeAF4Iw59EuSNAuzDYQbgbcAq4D9wCdavdcn+5qiPlWb10iyJclIkpHx8fEZdViSNLVZBUJVPVNVB6vqVeAzwJq2aAxY1rXqUmBfqy/tUT+sTZLFwKkc4RRVVW2vqtVVtXpoaGg2XZckHcGsAqFdE5jwXmDiDqRdwHC7c+g8OhePH6yq/cBLSda26wMbgbu62mxq85cD97frDJKkebR4uhWSfA64BDgzyRjwUeCSJKvonNp5Eng/QFXtTbITeBQ4AGytqoNtU1fRuWPpZOCeNgHcBNyWZJTOkcHwUdgvSdIMTRsIVXVlj/JNU6y/DdjWoz4CXNij/jJwxXT9kCQdW35TWZIEGAiSpMZAkCQBBoIkqTEQJEmAgSBJagwESRJgIEiSGgNBkgQYCJKkxkCQJAEGgiSpMRAkSYCBIElqDARJEmAgSJIaA0GSBBgIkqTGQJAkAQaCJKkxECRJgIEgSWoMBEkSYCBIkhoDQZIEGAiSpGbaQEhyc5JnkzzSVTs9yb1JvtceT+tadk2S0SSPJ7m0q35xkj1t2fVJ0uonJbmj1XcnWX6U91GS1Id+jhBuAdZPql0N3FdVK4D72nOSrASGgQtamxuSLGptbgS2ACvaNLHNzcDzVXU+cB1w7Wx3RpI0e9MGQlV9HXhuUnkDsKPN7wAu66rfXlWvVNUTwCiwJsk5wClV9UBVFXDrpDYT27oTWDdx9CBJmj+zvYZwdlXtB2iPZ7X6EuDprvXGWm1Jm59cP6xNVR0AXgDO6PWHJtmSZCTJyPj4+Cy7Lknq5WhfVO71yb6mqE/V5rXFqu1VtbqqVg8NDc2yi5KkXmYbCM+000C0x2dbfQxY1rXeUmBfqy/tUT+sTZLFwKm89hSVJOkYm20g7AI2tflNwF1d9eF259B5dC4eP9hOK72UZG27PrBxUpuJbV0O3N+uM0iS5tHi6VZI8jngEuDMJGPAR4GPAzuTbAaeAq4AqKq9SXYCjwIHgK1VdbBt6io6dyydDNzTJoCbgNuSjNI5Mhg+KnsmSZqRaQOhqq48wqJ1R1h/G7CtR30EuLBH/WVaoEiSBsdvKkuSAANBktQYCJIkwECQJDUGgiQJMBAkSY2BIEkCDARJUmMgSJIAA0GS1BgIkiTAQJAkNQaCJAkwECRJjYEgSQIMBElSYyBIkgADQZLUGAiSJMBAkCQ1BoIkCTAQJEmNgSBJAgwESVJjIEiSgDkGQpInk+xJ8nCSkVY7Pcm9Sb7XHk/rWv+aJKNJHk9yaVf94rad0STXJ8lc+iVJmrmjcYTwzqpaVVWr2/OrgfuqagVwX3tOkpXAMHABsB64Icmi1uZGYAuwok3rj0K/JEkzcCxOGW0AdrT5HcBlXfXbq+qVqnoCGAXWJDkHOKWqHqiqAm7taiNJmidzDYQC/jXJQ0m2tNrZVbUfoD2e1epLgKe72o612pI2P7n+Gkm2JBlJMjI+Pj7HrkuSui2eY/t3VNW+JGcB9yb57hTr9rouUFPUX1us2g5sB1i9enXPdSRJszOnI4Sq2tcenwW+CKwBnmmngWiPz7bVx4BlXc2XAvtafWmPuiRpHs06EJK8IcmvT8wDfww8AuwCNrXVNgF3tfldwHCSk5KcR+fi8YPttNJLSda2u4s2drWRJM2TuZwyOhv4YrtDdDHwT1X1lSTfAnYm2Qw8BVwBUFV7k+wEHgUOAFur6mDb1lXALcDJwD1tkiTNo1kHQlV9H3hbj/pPgHVHaLMN2NajPgJcONu+SJLmzm8qS5IAA0GS1BgIkiTAQJAkNQaCJAkwECRJjYEgSQIMBElSYyBIkgADQZLUGAiSJMBAkCQ1BoIkCTAQJEmNgSBJAgwESVJjIEiSAANBktQYCJIkwECQJDUGgiQJMBAkSY2BIEkCDARJUmMgSJIAA0GS1CyYQEiyPsnjSUaTXD3o/kjSiWZBBEKSRcCngHcBK4Erk6wcbK8k6cSyIAIBWAOMVtX3q+qXwO3AhgH3SZJOKKmqQfeBJJcD66vqz9rz9wFvr6oPTFpvC7ClPf1N4PF57WhvZwI/HnQnFgjHosNxOMSxOGShjMWbq2qo14LF892TI0iP2muSqqq2A9uPfXf6l2SkqlYPuh8LgWPR4Tgc4lgccjyMxUI5ZTQGLOt6vhTYN6C+SNIJaaEEwreAFUnOS/IrwDCwa8B9kqQTyoI4ZVRVB5J8APgXYBFwc1XtHXC3+rWgTmENmGPR4Tgc4lgcsuDHYkFcVJYkDd5COWUkSRowA0GSBBgIkqTGQJAkAQvkLqPjSZKzgSV0vji3r6qeGXCXBsaxUC++Lg453sbCu4z6lGQV8PfAqcAPW3kp8FPgL6rq24Pp2fxzLHo73t78R5uvi0OO17EwEPqU5GHg/VW1e1J9LfDpqnrbQDo2AI7F4Y7XN//R5uvikON1LDxl1L83TP7LBaiqbyZ5wyA6NECOxeFu4chv/n8EFuSb/xjwdXHIcTkWBkL/7knyZeBW4OlWWwZsBL4ysF4NhmNxuOPyzX8M+Lo45LgcC08ZzUCSd9H5nYYldP6H1jFgV1XdPdCODYBjcUiS64G30PvN/8Tk/8b9/zNfF4ccj2NhIEhHwfH45pcmMxCOgiRb2m81nPAcC/Xi6+KQhTwWfjHt6Oj1Az8nKseiS/uVP/m66LZgx8KLynOQ5Naq2lhVnx50X+ZbkjVAVdW3kqwE1gPfPRHHYhoL9s1/LCT5LTqnzXZX1c+6Fv1gQF1aEJL8Pp3fjn9kIb9HDIQ+JZn8gz0B3pnkTQBV9afz3qkBSfJR4F3A4iT3Am8HvgZcneSiqto2yP4tML8cdAfmS5K/BLYCjwE3JflgVd3VFv8tC/jumqMtyYNVtabN/zmdcfki8NEkv1tVHx9oB4/Aawh9SvJt4FHgH+h8EzXA5+j8uhtV9W+D6938SrIHWAWcBPwIWFpVLyY5mc4nw98ZZP8WkiRPVdW5g+7HfGivi9+rqp8lWQ7cCdxWVX+X5D+q6qLB9nD+dO9vkm8B766q8XYb8jer6rcH28PePELo32rgg8BfA39VVQ8n+cWJFARdDlTVQeDnSf6rql4EqKpfJHl1wH2bd0m+c6RFwNnz2ZcBWzRxmqiqnkxyCXBnkjdzgp06A16X5DQ612lTVeMAVfXfSQ4MtmtHZiD0qapeBa5L8s/t8RlO3PH7ZZJfq6qfAxdPFJOcCpxwgUDnH/1Lgecn1QP8+/x3Z2B+lGRVVT0M0I4U/gS4GViQn4iPoVOBh+i8BirJb1TVj5K8kQUcjifqP2izVlVjwBVJ3gO8OOj+DMgfVNUr8H9BOeH1wKbBdGmgvgS8ceIfwm5JvjbvvRmcjcBhn36r6gCwMcmCvZB6LFTV8iMsehV47zx2ZUa8hiBJAvwegiSpMRAkSYCBIElqDARJEgD/C2fVMCohBuF3AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# perform undersampling to get balanced dataset\n",
    "\n",
    "least_class_amount, class_1, class_3, class_4, class_5 = combined.reviews_rating.value_counts(ascending=True)\n",
    "\n",
    "# create separate dataframes for each rating\n",
    "c5 = combined[combined[\"reviews_rating\"] == 5.0]\n",
    "c4 = combined[combined[\"reviews_rating\"] == 4.0]\n",
    "c3 = combined[combined[\"reviews_rating\"] == 3.0]\n",
    "c2 = combined[combined[\"reviews_rating\"] == 2.0]\n",
    "c1 = combined[combined[\"reviews_rating\"] == 1.0]\n",
    "\n",
    "# randomly remove data to in each dataframe to match the number of rows in c2 which has the least amount of data\n",
    "df_5 = c5.sample(least_class_amount, random_state=50)\n",
    "df_4 = c4.sample(least_class_amount, random_state=50)\n",
    "df_3 = c3.sample(least_class_amount, random_state=50)\n",
    "df_1 = c1.sample(least_class_amount, random_state=50)\n",
    "\n",
    "# concatenate all 5 dataframes into 1\n",
    "undersampled = pd.concat([df_5, df_4, df_3, c2, df_1], axis=0)\n",
    "combined = undersampled\n",
    "combined.reviews_rating.value_counts().plot.bar()\n",
    "combined.reviews_rating.value_counts()\n",
    "combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove non-ascii characters\n",
    "combined[\"reviews_text\"] = combined[\"reviews_text\"].str.encode('ascii', 'ignore').str.decode('ascii')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviews_rating</th>\n",
       "      <th>reviews_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18203</th>\n",
       "      <td>2</td>\n",
       "      <td>super convenient battery life for weeks perfec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14901</th>\n",
       "      <td>2</td>\n",
       "      <td>my children love these kindles they are so muc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>344876</th>\n",
       "      <td>2</td>\n",
       "      <td>i have to say i had a few choices to choose fr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>540077</th>\n",
       "      <td>2</td>\n",
       "      <td>phenomenal bbq combined with fast service and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1024969</th>\n",
       "      <td>2</td>\n",
       "      <td>the beef noodle soup is the star you cant get ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>552208</th>\n",
       "      <td>0</td>\n",
       "      <td>dont waste time or money i live  miles away fr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1069965</th>\n",
       "      <td>0</td>\n",
       "      <td>translated by google we went here to eat the n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>484556</th>\n",
       "      <td>0</td>\n",
       "      <td>we were anticipating a delightful experience a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>401200</th>\n",
       "      <td>0</td>\n",
       "      <td>th dennys is officially ghetto\\nhave been here...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1072684</th>\n",
       "      <td>0</td>\n",
       "      <td>when i asked if i could substitute on the meal...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150460 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         reviews_rating                                       reviews_text\n",
       "18203                 2  super convenient battery life for weeks perfec...\n",
       "14901                 2  my children love these kindles they are so muc...\n",
       "344876                2  i have to say i had a few choices to choose fr...\n",
       "540077                2  phenomenal bbq combined with fast service and ...\n",
       "1024969               2  the beef noodle soup is the star you cant get ...\n",
       "...                 ...                                                ...\n",
       "552208                0  dont waste time or money i live  miles away fr...\n",
       "1069965               0  translated by google we went here to eat the n...\n",
       "484556                0  we were anticipating a delightful experience a...\n",
       "401200                0  th dennys is officially ghetto\\nhave been here...\n",
       "1072684               0  when i asked if i could substitute on the meal...\n",
       "\n",
       "[150460 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using regular expressions to remove unwanted characters and punctuations\n",
    "import re\n",
    "\n",
    "def cleanup(sentence):\n",
    "  sentence = str(sentence).lower()\n",
    "  sentence = cleanup_re.sub('',sentence).strip() \n",
    "  sentence = re.sub('[\\d]','',sentence) # remove all digits and numbers\n",
    "  return sentence\n",
    "\n",
    "def to_int(rating):\n",
    "    rating = int(rating) - 1\n",
    "    if rating >= 3:\n",
    "      rating = 2 # positive\n",
    "    elif rating <= 1:\n",
    "      rating = 0 # negative\n",
    "    else:\n",
    "      rating = 1\n",
    "    return rating # neutral\n",
    "  \n",
    "cleanup_re = re.compile('[^\\w\\s]') # match whitespaces words only\n",
    "combined[\"reviews_text\"] = combined[\"reviews_text\"].apply(cleanup)\n",
    "combined[\"reviews_rating\"] = combined[\"reviews_rating\"].apply(to_int)\n",
    "combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviews_rating</th>\n",
       "      <th>reviews_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18203</th>\n",
       "      <td>2</td>\n",
       "      <td>super convenient battery life weeks perfect mo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14901</th>\n",
       "      <td>2</td>\n",
       "      <td>children love kindles much better st ones had ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>344876</th>\n",
       "      <td>2</td>\n",
       "      <td>have say had choices choose dinner tonight hap...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>540077</th>\n",
       "      <td>2</td>\n",
       "      <td>phenomenal bbq combined fast service lots room...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1024969</th>\n",
       "      <td>2</td>\n",
       "      <td>beef noodle soup star cant get dish anywhere e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>552208</th>\n",
       "      <td>0</td>\n",
       "      <td>dont waste time money live miles away restaura...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1069965</th>\n",
       "      <td>0</td>\n",
       "      <td>translated google went eat nola gumbo ordered ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>484556</th>\n",
       "      <td>0</td>\n",
       "      <td>anticipating delightful experience restaurant ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>401200</th>\n",
       "      <td>0</td>\n",
       "      <td>th dennys officially ghetto have min no food s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1072684</th>\n",
       "      <td>0</td>\n",
       "      <td>asked could substitute meal told no realize di...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150460 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         reviews_rating                                       reviews_text\n",
       "18203                 2  super convenient battery life weeks perfect mo...\n",
       "14901                 2  children love kindles much better st ones had ...\n",
       "344876                2  have say had choices choose dinner tonight hap...\n",
       "540077                2  phenomenal bbq combined fast service lots room...\n",
       "1024969               2  beef noodle soup star cant get dish anywhere e...\n",
       "...                 ...                                                ...\n",
       "552208                0  dont waste time money live miles away restaura...\n",
       "1069965               0  translated google went eat nola gumbo ordered ...\n",
       "484556                0  anticipating delightful experience restaurant ...\n",
       "401200                0  th dennys officially ghetto have min no food s...\n",
       "1072684               0  asked could substitute meal told no realize di...\n",
       "\n",
       "[150460 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove stopwords \n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def remove_stopwords(sentence):\n",
    "  sentence = sentence.split()\n",
    "  sentence = [word for word in sentence if word not in stoplist]\n",
    "  sentence = ' '.join(sentence)\n",
    "  return sentence\n",
    "    \n",
    "\n",
    "def custom_stoplist():\n",
    "  stop_words = set(stopwords.words('english'))\n",
    "  wanted_stopwords = {'not', 'nor', 'no', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'very'} # stopwords that can change a sentence's meaning\n",
    "  stoplist = stop_words - wanted_stopwords\n",
    "  return stoplist\n",
    "\n",
    "stoplist = custom_stoplist()\n",
    "combined[\"reviews_text\"] = combined[\"reviews_text\"].apply(remove_stopwords)\n",
    "combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined[\"reviews_rating\"].value_counts().plot.bar()\n",
    "combined[\"reviews_rating\"].value_counts(ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150460\n"
     ]
    }
   ],
   "source": [
    "# perform 80-20 split on data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "sentences = combined[\"reviews_text\"].tolist()\n",
    "labels = combined[\"reviews_rating\"].tolist()\n",
    "print(len(sentences))\n",
    "training_sentences, test_sentences, training_labels, test_labels = train_test_split(sentences, labels, test_size=0.2, train_size=0.8, random_state=35) \n",
    "# random_state controls how data is being shuffled\n",
    "# ensures that data is being shuffled the same way every time the cell is being ran"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructing the Embedding Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91284\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "vocab_size = 40000 # number of words that will be fed into model #91334\n",
    "max_length = 100 #\n",
    "trunc_type='post' # \n",
    "padding_type='post' #\n",
    "oov_tok = \"<OOV>\" # placeholder if model encounters words it has never seen before in the test set\n",
    "\n",
    "# Tokenization - breaking down sentences into its individual words and assigning a number to it\n",
    "tokenizer = Tokenizer(num_words=vocab_size, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',oov_token=oov_tok)\n",
    "tokenizer.fit_on_texts(training_sentences)\n",
    "word_index = tokenizer.word_index # Model's 'vocabulary' learnt from training data (corpus)\n",
    "vocab_length = len(word_index) + 1\n",
    "print(vocab_length)\n",
    "\n",
    "# Sequencing - converting each sentence into its numerical equivalent\n",
    "# Padding and truncating used to make all sentences same length \n",
    "training_sequences = tokenizer.texts_to_sequences(training_sentences) # convert each word to its respective index of the corpus\n",
    "training_padded = pad_sequences(training_sequences, maxlen=max_length, padding=padding_type,\n",
    "                               truncating=trunc_type) # add padding to sequence to ensure 100 index length\n",
    "\n",
    "test_sequences = tokenizer.texts_to_sequences(test_sentences)\n",
    "test_padded = pad_sequences(test_sequences, maxlen=max_length, padding=padding_type,\n",
    "                               truncating=trunc_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since neural nets do not perform well on sparse data, sequences need to be converted to vectors instead of binary values \n",
    "# Use GloVe word embeddings \n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "\n",
    "embeddings_dict = {}\n",
    "embeddings_file = open('E:\\GloVe Word Embeddings\\glove.6B.100d.txt', encoding='utf8')\n",
    "\n",
    "for line in embeddings_file:\n",
    "  records = line.split()\n",
    "  word = records[0]\n",
    "  vector_dimensions = np.asarray(records[1:], dtype='float32')\n",
    "  embeddings_dict[word] = vector_dimensions\n",
    "\n",
    "embeddings_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(91284, 100)\n"
     ]
    }
   ],
   "source": [
    "# create embedding matrix by assigning the word vector in GloVe file to the corresponding word from the corpus\n",
    "embedding_matrix = np.zeros((vocab_length, 100))\n",
    "for word, index in word_index.items():\n",
    "  embedding_vector = embeddings_dict.get(word)\n",
    "  if embedding_vector is not None:\n",
    "    embedding_matrix[index] = embedding_vector\n",
    "print(embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  to categorical\n",
    "training_labels = tf.keras.utils.to_categorical(training_labels, num_classes=None)\n",
    "test_labels = tf.keras.utils.to_categorical(test_labels, num_classes=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(test_labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building (Simple NN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # perform one-hot encoding on sequences - convert sequences into binary values\n",
    "# training_padded = np.array(training_padded)\n",
    "# training_labels = tf.keras.utils.to_categorical(training_labels)\n",
    "# test_padded = np.array(test_padded)\n",
    "# test_labels = tf.keras.utils.to_categorical(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(training_padded)\n",
    "# print(training_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_of_ratings = 6 # number of possible ratings given by users: 1 to 5\n",
    "# embedding_dim = 32\n",
    "# # Creating a model with 3 fully connected layers:\n",
    "# # 1 embedding layer to find correlation between words, 1 pooling layer, 1 layer with relu function \n",
    "# model = tf.keras.Sequential([\n",
    "#     tf.keras.layers.Embedding(vocab_length, embedding_dim, input_length=maxlen),\n",
    "#     tf.keras.layers.GlobalAveragePooling1D(),\n",
    "#     tf.keras.layers.Dense(24, activation='relu'), \n",
    "#     tf.keras.layers.Dense(num_of_ratings, activation='softmax')\n",
    "# ]) \n",
    "# model.compile(optimizer='rmsprop',loss='categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training model\n",
    "\n",
    "#epochs_2 = 10\n",
    "#prediction_2 = model.fit(training_padded, training_labels, epochs=epochs_2, validation_data=(test_padded, test_labels), verbose=2)\n",
    "\n",
    "# epochs_2 = 70\n",
    "# history = model.fit(training_padded, training_labels, epochs=epochs_2, validation_data=(test_padded, test_labels), verbose=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence = [\"food took too long to come but overall experience was ok\"]\n",
    "# sequences = tokenizer.texts_to_sequences(sentence)\n",
    "# padded = pad_sequences(sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
    "# output= model.predict(padded)\n",
    "\n",
    "# print(output)\n",
    "# print(\"Review:\", sentence)\n",
    "# print(\"Rating:\", np.argmax(output, axis=None, out=None))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building & Training (LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM model architecture\n",
    "from keras.models import Sequential \n",
    "from keras.layers.core import Activation, Dense\n",
    "from keras.layers import Embedding, LSTM\n",
    "\n",
    "embedding_layer = tf.keras.layers.Embedding(vocab_length, 100, weights=[embedding_matrix], input_length=max_length, trainable=False)\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
    "model = Sequential()\n",
    "model.add(embedding_layer)\n",
    "model.add(tf.keras.layers.LSTM(128, dropout=0.3))\n",
    "model.add(tf.keras.layers.Dense(3, activation='softmax'))\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.debugging.set_log_device_placement(True)\n",
    "history = model.fit(training_padded, training_labels, batch_size=64, callbacks=[callback], epochs=30, verbose=1, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(test_padded, test_labels, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot model performance charts \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train','test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train','test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "model.save(f\"E:\\Saved_NLP_models\\c1_lstm_model_acc_{round(score[1], 3)}.h5\", save_format='h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras tuner "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keras tuner\n",
    "from keras.models import Sequential \n",
    "from keras.layers.core import Activation, Dense\n",
    "from keras.layers import Embedding, LSTM\n",
    "import keras_tuner as kt\n",
    "\n",
    "def model_building (hp):\n",
    "\n",
    "    embedding_dims = hp.Int('output_dim', min_value=8, max_value=128, step=8)\n",
    "    dense_units = hp.Int('units', min_value=8, max_value=128, step=8)\n",
    "    dropout = hp.Int('dropout', min_value=8, max_value=128, step=8)\n",
    "    lstm_units = hp.Int('units', min_value=8, max_value=128, step=8)\n",
    "\n",
    "    embedding_layer = tf.keras.layers.Embedding(vocab_length, embedding_dims=embedding_dims, weights=[embedding_matrix], input_length=max_length, trainable=False)\n",
    "    model = Sequential()\n",
    "    model.add(embedding_layer)\n",
    "    model.add(tf.keras.layers.LSTM(units=lstm_units, dropout=dropout))\n",
    "    model.add(tf.keras.layers.Dense(units=dense_units, activation='softmax'))\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = kt.Hyperband(model_building,\n",
    "                     objective='val_accuracy',\n",
    "                     max_epochs=10,\n",
    "                     factor=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter optimisation \n",
    "tuner.search(training_padded, training_labels, epochs=50, validation_data=(test_padded, test_labels), callbacks=[callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retreiving best hyper params\n",
    "best_hps_rs=tuner.get_best_hyperparameters()[0]\n",
    "# Creating model with hyper params \n",
    "hypermodel_rs = tuner.hypermodel.build(best_hps_rs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = hypermodel_rs.fit(training_padded, training_labels, epochs=10, validation_data=(test_padded, test_labels), callbacks=[callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best epoch (OPTIONAL)\n",
    "val_acc_per_epoch = history.history['val_accuracy']\n",
    "best_epoch = val_acc_per_epoch.index(max(val_acc_per_epoch)) + 1\n",
    "print('Best epoch: %d' % (best_epoch,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rerun with the best epoch (OPTIONAL)\n",
    "history = hypermodel_rs.fit(training_padded, training_labels, epochs=best_epoch, validation_data=(test_padded, test_labels), callbacks=[callback])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Model on Unseen Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing model on unseen data \n",
    "\n",
    "unseen = pd.read_csv(r'E:\\AIEBA Datasets\\Proj1 Datasets\\unseen_data.csv',encoding='utf-8',engine='python')\n",
    "unseen[\"reviews_text\"] = unseen[\"reviews_text\"].apply(cleanup)\n",
    "unseen_reviews = unseen[\"reviews_text\"].tolist()\n",
    "unseen_tokenized = tokenizer.texts_to_sequences(unseen_reviews)\n",
    "unseen_padded = pad_sequences(unseen_tokenized, padding='post',maxlen=max_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_ratings = []\n",
    "sentiments = model.predict(unseen_padded)\n",
    "for sentiment in sentiments:\n",
    "  predicted_ratings.append(np.argmax(sentiment))\n",
    "\n",
    "unseen[\"predicted_ratings\"] = predicted_ratings\n",
    "unseen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = [\"service was good and food was insanely good\"]\n",
    "sequences = tokenizer.texts_to_sequences(sentence)\n",
    "padded = pad_sequences(sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
    "output= model.predict(padded)\n",
    "\n",
    "print(output)\n",
    "print(\"Review:\", sentence)\n",
    "print(\"Rating:\", np.argmax(output, axis=None, out=None))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f8b40d688a12481f01eadf7380c47edd8a49484a47dba3db091451640e880c68"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
